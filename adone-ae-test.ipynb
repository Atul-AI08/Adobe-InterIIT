{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":5695982,"sourceType":"datasetVersion","datasetId":3274841}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n        super().__init__()\n        # This combines the Wq, Wk and Wv matrices into one matrix\n        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n        # This one represents the Wo matrix\n        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n        self.n_heads = n_heads\n        self.d_head = d_embed // n_heads\n\n    def forward(self, x, causal_mask=False):\n        # x: # (Batch_Size, Seq_Len, Dim)\n\n        # (Batch_Size, Seq_Len, Dim)\n        input_shape = x.shape \n        \n        # (Batch_Size, Seq_Len, Dim)\n        batch_size, sequence_length, d_embed = input_shape \n\n        # (Batch_Size, Seq_Len, H, Dim / H)\n        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head) \n\n        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n        \n        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n        q = q.view(interim_shape).transpose(1, 2)\n        k = k.view(interim_shape).transpose(1, 2)\n        v = v.view(interim_shape).transpose(1, 2)\n\n        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n        weight = q @ k.transpose(-1, -2)\n        \n        if causal_mask:\n            # Mask where the upper triangle (above the principal diagonal) is 1\n            mask = torch.ones_like(weight, dtype=torch.bool).triu(1) \n            # Fill the upper triangle with -inf\n            weight.masked_fill_(mask, -torch.inf) \n        \n        # Divide by d_k (Dim / H). \n        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n        weight /= math.sqrt(self.d_head) \n\n        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n        weight = F.softmax(weight, dim=-1) \n\n        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n        output = weight @ v\n\n        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n        output = output.transpose(1, 2) \n\n        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n        output = output.reshape(input_shape) \n\n        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n        output = self.out_proj(output) \n        \n        # (Batch_Size, Seq_Len, Dim)\n        return output\n\nclass CrossAttention(nn.Module):\n    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n        super().__init__()\n        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n        self.n_heads = n_heads\n        self.d_head = d_embed // n_heads\n    \n    def forward(self, x, y):\n        # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n        # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n\n        input_shape = x.shape\n        batch_size, sequence_length, d_embed = input_shape\n        # Divide each embedding of Q into multiple heads such that d_heads * n_heads = Dim_Q\n        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n        \n        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n        q = self.q_proj(x)\n        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n        k = self.k_proj(y)\n        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n        v = self.v_proj(y)\n\n        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n        q = q.view(interim_shape).transpose(1, 2) \n        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n        k = k.view(interim_shape).transpose(1, 2) \n        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n        v = v.view(interim_shape).transpose(1, 2) \n        \n        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) @ (Batch_Size, H, Dim_Q / H, Seq_Len_KV) -> (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n        weight = q @ k.transpose(-1, -2)\n        \n        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n        weight /= math.sqrt(self.d_head)\n        \n        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n        weight = F.softmax(weight, dim=-1)\n        \n        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV) @ (Batch_Size, H, Seq_Len_KV, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n        output = weight @ v\n        \n        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H)\n        output = output.transpose(1, 2).contiguous()\n        \n        # (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n        output = output.view(input_shape)\n        \n        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n        output = self.out_proj(output)\n\n        # (Batch_Size, Seq_Len_Q, Dim_Q)\n        return output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-04T07:40:09.150444Z","iopub.execute_input":"2024-12-04T07:40:09.151355Z","iopub.status.idle":"2024-12-04T07:40:09.166269Z","shell.execute_reply.started":"2024-12-04T07:40:09.151309Z","shell.execute_reply":"2024-12-04T07:40:09.165640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorly","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:09.167514Z","iopub.execute_input":"2024-12-04T07:40:09.167760Z","iopub.status.idle":"2024-12-04T07:40:18.526041Z","shell.execute_reply.started":"2024-12-04T07:40:09.167742Z","shell.execute_reply":"2024-12-04T07:40:18.524845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:18.527452Z","iopub.execute_input":"2024-12-04T07:40:18.527770Z","iopub.status.idle":"2024-12-04T07:40:27.672478Z","shell.execute_reply.started":"2024-12-04T07:40:18.527746Z","shell.execute_reply":"2024-12-04T07:40:27.671608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image,ImageFile,ImageOps\nimport os\nimport torch\nimport tensorly as tl\nimport cv2\nfrom tensorly.decomposition import tucker,parafac\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom einops import rearrange, reduce, repeat\nfrom einops.layers.torch import Rearrange, Reduce\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,Dataset,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\nimport torch.optim as optim\nimport copy\nfrom sklearn.model_selection import KFold\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom scipy.signal import wiener\ntorch.set_num_threads(1)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:27.674594Z","iopub.execute_input":"2024-12-04T07:40:27.674894Z","iopub.status.idle":"2024-12-04T07:40:29.219356Z","shell.execute_reply.started":"2024-12-04T07:40:27.674870Z","shell.execute_reply":"2024-12-04T07:40:29.218680Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pickle\nimport copy\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n     \n            self.counter = 0\n    ","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.220368Z","iopub.execute_input":"2024-12-04T07:40:29.220612Z","iopub.status.idle":"2024-12-04T07:40:29.227425Z","shell.execute_reply.started":"2024-12-04T07:40:29.220592Z","shell.execute_reply":"2024-12-04T07:40:29.226677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VAE_AttentionBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(32, channels)\n        self.attention = SelfAttention(1, channels)\n    \n    def forward(self, x):\n        # x: (Batch_Size, Features, Height, Width)\n\n        residue = x \n\n        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n        x = self.groupnorm(x)\n\n        n, c, h, w = x.shape\n        \n        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n        x = x.view((n, c, h * w))\n        \n        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features). Each pixel becomes a feature of size \"Features\", the sequence length is \"Height * Width\".\n        x = x.transpose(-1, -2)\n        \n        # Perform self-attention WITHOUT mask\n        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n        x = self.attention(x)\n        \n        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n        x = x.transpose(-1, -2)\n        \n        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n        x = x.view((n, c, h, w))\n        \n        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width) \n        x += residue\n\n        # (Batch_Size, Features, Height, Width)\n        return x \n\nclass VAE_ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n\n        if in_channels == out_channels:\n            self.residual_layer = nn.Identity()\n        else:\n            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n    \n    def forward(self, x):\n        # x: (Batch_Size, In_Channels, Height, Width)\n\n        residue = x\n\n        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n        x = self.groupnorm_1(x)\n        \n        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n        x = F.silu(x)\n        \n        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n        x = self.conv_1(x)\n        \n        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n        x = self.groupnorm_2(x)\n        \n        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n        x = F.silu(x)\n        \n        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n        x = self.conv_2(x)\n        \n        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n        return x + self.residual_layer(residue)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.228724Z","iopub.execute_input":"2024-12-04T07:40:29.228946Z","iopub.status.idle":"2024-12-04T07:40:29.246745Z","shell.execute_reply.started":"2024-12-04T07:40:29.228929Z","shell.execute_reply":"2024-12-04T07:40:29.246115Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VAE_Encoder(nn.Sequential):\n    def __init__(self):\n        super().__init__(\n            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            \n             # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n            VAE_ResidualBlock(32, 32),\n            \n            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height / 2, Width / 2)\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0), \n            \n            # (Batch_Size, 256, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 2, Width / 2)\n            VAE_ResidualBlock(32,64), \n            \n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0), \n            \n            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n            VAE_ResidualBlock(64, 64), \n            VAE_ResidualBlock(64, 64), \n            \n            VAE_AttentionBlock(64), \n            \n            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n            VAE_ResidualBlock(64, 64), \n            \n            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n            nn.GroupNorm(32, 64), \n            \n            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n            nn.SiLU(), \n\n            nn.Conv2d(64, 64,kernel_size=3, padding=1)\n        )\n\n    def forward(self, x):\n        # x: (Batch_Size, Channel, Height, Width)\n        # noise: (Batch_Size, 4, Height / 8, Width / 8)\n\n        for module in self:\n\n            if getattr(module, 'stride', None) == (2, 2):  # Padding at downsampling should be asymmetric (see #8)\n                # Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\n                # Pad with zeros on the right and bottom.\n                # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\n                x = F.pad(x, (0, 1, 0, 1))\n            \n            x = module(x)\n            \n             # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\n        mean, log_variance = torch.chunk(x, 2, dim=1)\n        log_variance = torch.clamp(log_variance, -30, 20)\n        variance = log_variance.exp()\n        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n        stdev = variance.sqrt()\n        \n        noise = torch.randn_like(mean)\n        x = mean + stdev * noise\n        \n        # Scale by a constant\n        # Constant taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L17C1-L17C\n        \n        return x\n        # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.247825Z","iopub.execute_input":"2024-12-04T07:40:29.248182Z","iopub.status.idle":"2024-12-04T07:40:29.267000Z","shell.execute_reply.started":"2024-12-04T07:40:29.248156Z","shell.execute_reply":"2024-12-04T07:40:29.266370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n\tdef __init__(self, in_channels, patch_size, emb_size, img_size):#in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224\n\t\tsuper().__init__()\n\t\tself.patch_size = patch_size\n\t\tself.nPatches = (img_size*img_size)//((patch_size)**2)\n\t\tself.projection = nn.Sequential(\n\t\t\tRearrange('b c (h p1)(w p2) -> b (h w) (p1 p2 c)',p1 = patch_size,p2 = patch_size),\n\t\t\tnn.Linear(patch_size * patch_size * in_channels, emb_size)\n\t\t)\n\t\tself.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n\t\t#self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1,emb_size))\n                \n\tdef forward(self, x):\n\t\tb,c,h,w = x.shape\n\t\tx = self.projection(x)\n\t\tcls_tokens = repeat(self.cls_token,'() n e -> b n e', b=b)#repeat the cls tokens for all patch set in \n\t\tx = torch.cat([cls_tokens,x],dim=1)\n\t\t#x+=self.positions\n\t\treturn x","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.267912Z","iopub.execute_input":"2024-12-04T07:40:29.268149Z","iopub.status.idle":"2024-12-04T07:40:29.282302Z","shell.execute_reply.started":"2024-12-04T07:40:29.268132Z","shell.execute_reply":"2024-12-04T07:40:29.281732Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class multiHeadAttention(nn.Module):\n\tdef __init__(self, emb_size, heads, dropout):\n\t\tsuper().__init__()\n\t\tself.heads = heads\n\t\tself.emb_size = emb_size\n\t\tself.query = nn.Linear(emb_size,emb_size)\n\t\tself.key = nn.Linear(emb_size,emb_size)\n\t\tself.value = nn.Linear(emb_size,emb_size)\n\t\tself.drop_out = nn.Dropout(dropout)\n\t\tself.projection = nn.Linear(emb_size,emb_size)\n\n\tdef forward(self,x):\n\t\t#splitting the single input int number of heads\n\t\tqueries = rearrange(self.query(x),\"b n (h d) -> b h n d\", h = self.heads)\n\t\tkeys = rearrange(self.key(x),\"b n (h d) -> b h n d\", h = self.heads)\n\t\tvalues = rearrange(self.value(x),\"b n (h d) -> b h n d\", h = self.heads)\n\t\tattention_maps = torch.einsum(\"bhqd, bhkd -> bhqk\",queries,keys)\n\t\tscaling_value = self.emb_size**(1/2)\n\t\tattention_maps = F.softmax(attention_maps,dim=-1)/scaling_value\n\t\tattention_maps = self.drop_out(attention_maps)##might be deleted\n\t\toutput = torch.einsum(\"bhal, bhlv -> bhav\",attention_maps,values)\n\t\toutput  = rearrange(output,\"b h n d -> b n (h d)\")\n\t\toutput = self.projection(output)\n\t\treturn output\nclass residual(nn.Module):\n\tdef __init__(self,fn):\n\t\tsuper().__init__()\n\t\tself.fn = fn\n\tdef forward(self,x):\n\t\tidentity = x\n\t\tres = self.fn(x)\n\t\tout = res + identity\n\t\treturn out","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.283329Z","iopub.execute_input":"2024-12-04T07:40:29.283919Z","iopub.status.idle":"2024-12-04T07:40:29.295813Z","shell.execute_reply.started":"2024-12-04T07:40:29.283891Z","shell.execute_reply":"2024-12-04T07:40:29.295176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DeepBlock(nn.Sequential):\n\tdef __init__(self,emb_size:int =256 ,drop_out:float=0.0):#64\n\t\tsuper().__init__(\n        \t\tresidual(\n            \t\t\tnn.Sequential(\n                \t\t\tnn.LayerNorm(emb_size),\n                \t\t\tmultiHeadAttention(emb_size,2,drop_out),\n                \t\t\tnn.LayerNorm(emb_size)\n            \t\t\t)\n        \t\t)\n    \t\t)\n\nclass Classification(nn.Sequential):\n\tdef __init__(self, emb_size:int=256, n_classes:int=2):\n\t\tsuper().__init__(\n\t\t\t# Reduce('b n e -> b e', reduction='mean'),\n            nn.Dropout(0.01),\n\t\t\tnn.LayerNorm(emb_size), \n\t\t\tnn.Linear(emb_size, n_classes))","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:13:19.175090Z","iopub.execute_input":"2024-12-04T08:13:19.175429Z","iopub.status.idle":"2024-12-04T08:13:19.181131Z","shell.execute_reply.started":"2024-12-04T08:13:19.175408Z","shell.execute_reply":"2024-12-04T08:13:19.180266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbeddingVit(nn.Module):\n    def __init__(self, in_channels, patch_size, emb_dim, num_patches):\n        super().__init__()\n        self.patch_embedding = nn.Conv2d(\n            in_channels, \n            emb_dim, \n            kernel_size=patch_size, \n            stride=patch_size\n        )\n        self.num_patches = num_patches\n\n    def forward(self, x):\n        # x shape: [batch_size, channels, height, width]\n        patches = self.patch_embedding(x)\n        # patches shape: [batch_size, emb_dim, num_patches_h, num_patches_w]\n        patches = patches.flatten(2)  # flatten spatial dimensions\n        # patches shape: [batch_size, emb_dim, num_patches]\n        return patches.transpose(1, 2)  # [batch_size, num_patches, emb_dim]\n\nclass MultiHeadAttentionVit(nn.Module):\n    def __init__(self, emb_dim, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = emb_dim // num_heads\n        \n        self.query = nn.Linear(emb_dim, emb_dim)\n        self.key = nn.Linear(emb_dim, emb_dim)\n        self.value = nn.Linear(emb_dim, emb_dim)\n        \n        self.out_proj = nn.Linear(emb_dim, emb_dim)\n\n    def forward(self, x):\n        batch_size, num_patches, emb_dim = x.shape\n        \n        # Linear projections\n        Q = self.query(x).view(batch_size, num_patches, self.num_heads, self.head_dim)\n        K = self.key(x).view(batch_size, num_patches, self.num_heads, self.head_dim)\n        V = self.value(x).view(batch_size, num_patches, self.num_heads, self.head_dim)\n        \n        # Transpose for attention computation\n        Q = Q.transpose(1, 2)  # [batch_size, num_heads, num_patches, head_dim]\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n        \n        # Compute attention scores\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attention_probs = F.softmax(attention_scores, dim=-1)\n        \n        # Apply attention\n        context = torch.matmul(attention_probs, V)\n        \n        # Reshape and project\n        context = context.transpose(1, 2).contiguous().view(batch_size, num_patches, emb_dim)\n        return self.out_proj(context)\n\nclass TransformerBlockVit(nn.Module):\n    def __init__(self, emb_dim, num_heads, mlp_dim, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(emb_dim)\n        self.attn = MultiHeadAttentionVit(emb_dim, num_heads)\n        self.norm2 = nn.LayerNorm(emb_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, emb_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        # Multi-head self-attention\n        x = x + self.attn(self.norm1(x))\n        \n        # MLP\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass TinyViT(nn.Module):\n    def __init__(self, \n                 in_channels=3, \n                 patch_size=16, \n                 emb_dim=192, \n                 num_heads=3, \n                 num_layers=4, \n                 num_classes=1000,\n                 dropout=0.1,\n                 image_size=32):  # Add image_size as a parameter\n        super().__init__()\n        \n        # Dynamically calculate number of patches\n        num_patches = (image_size // patch_size) ** 2\n        \n        # Patch Embedding\n        self.patch_embed = PatchEmbeddingVit(\n            in_channels=in_channels, \n            patch_size=patch_size, \n            emb_dim=emb_dim, \n            num_patches=num_patches\n        )\n        \n        # Learnable class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n        \n        # Position embedding - now created dynamically\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n        \n        # Rest of the initialization remains the same\n         # Transformer Blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlockVit(\n                emb_dim=emb_dim, \n                num_heads=num_heads, \n                mlp_dim=emb_dim * 4, \n                dropout=dropout\n            ) for _ in range(num_layers)\n        ])\n        \n        # Layer Norm\n        self.norm = nn.LayerNorm(emb_dim)\n        \n        # Classification head\n        self.head = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        # Patch Embedding\n        x = self.patch_embed(x)\n        \n        # Add class token\n        batch_size = x.shape[0]\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        # Ensure positional embedding matches input size\n        if x.size(1) != self.pos_embed.size(1):\n            # Resize or adjust positional embedding if needed\n            pos_embed = F.interpolate(\n                self.pos_embed.transpose(1, 2), \n                size=x.size(1), \n                mode='linear', \n                align_corners=False\n            ).transpose(1, 2)\n        else:\n            pos_embed = self.pos_embed\n        \n        # Add positional embedding\n        x = x + pos_embed\n        \n        # Transformer Blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        # Normalize\n        x = self.norm(x)\n        \n        # Classification from class token\n        return x[:, 0] \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:13:19.379380Z","iopub.execute_input":"2024-12-04T08:13:19.379679Z","iopub.status.idle":"2024-12-04T08:13:19.395580Z","shell.execute_reply.started":"2024-12-04T08:13:19.379641Z","shell.execute_reply":"2024-12-04T08:13:19.394809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Model(nn.Module):\n  def __init__(self,emb_size,drop_out, n_classes,in_channels,patch_size1,patch_size2,image_size):\n    super().__init__()\n    self.encoder1 = VAE_Encoder()\n    self.encoder2 = TinyViT(\n                    in_channels=in_channels,\n        \n                    num_classes=n_classes)\n    # self.encoder = vae\n    self.PatchEmbedding1 = PatchEmbedding(32,patch_size1,emb_size,64)\n    self.PatchEmbedding2 = PatchEmbedding(32,patch_size2,emb_size,64)\n    self.DeepBlock = DeepBlock(emb_size = 16)#Transformer()\n    self.Classification = Classification(emb_size = 192,n_classes=2)\n  def forward(self,x):\n    x = self.encoder2(x)\n    # print(x.shape)\n      \n#     print(type(x))\n#     #x = x.view(16,32,56)\n#     patch1 = self.PatchEmbedding1(x)\n#     patch2 = self.PatchEmbedding2(x)\n# #     print(patch1.shape)\n# #     print(patch2.shape)\n# #      Resize tensor2 along the second dimension to match the size of tensor1\n#     desired_size = patch1.shape[1]  #patchEmbeddings1\n#     indices = torch.linspace(0, patch2.shape[1] - 1, desired_size, device=device).long()\n#     patch2_resized = torch.index_select(patch2, 1, indices)#.to(device)\n# #     print(patch2_resized.shape)\n\n#     # Concatenate the tensors along the second dimension (dim=1)\n#     patchEmbeddings = torch.cat((patch2_resized, patch1), dim=1)#.to(device)\n#     print(patchEmbeddings.shape)\n\n#     DeepBlockOp = self.DeepBlock(patchEmbeddings)\n#     print(DeepBlockOp.shape)\n    # classificationOutput = self.Classification(DeepBlockOp)\n    classificationOutput = self.Classification(x)\n#     print(classificationOutput.shape)\n    output = F.log_softmax(classificationOutput, dim=1)\n    return output","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:13:57.133041Z","iopub.execute_input":"2024-12-04T08:13:57.133777Z","iopub.status.idle":"2024-12-04T08:13:57.140068Z","shell.execute_reply.started":"2024-12-04T08:13:57.133750Z","shell.execute_reply":"2024-12-04T08:13:57.139235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_mag(f):\n  dm_frequency_domain = np.fft.fftshift(f)\n  dm_reduced_domain = dm_frequency_domain.copy()\n  # Set a threshold for magnitude to retain only the most significant coefficients\n  threshold = 0.001 * np.max(np.abs(dm_reduced_domain))\n  dm_reduced_domain[np.abs(dm_reduced_domain) < threshold] = 0\n  transformed_image = np.log(1 + np.abs(dm_reduced_domain))\n  img_filtered = torch.tensor(transformed_image)\n  return img_filtered\n    ","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:13:57.557860Z","iopub.execute_input":"2024-12-04T08:13:57.558195Z","iopub.status.idle":"2024-12-04T08:13:57.563658Z","shell.execute_reply.started":"2024-12-04T08:13:57.558171Z","shell.execute_reply":"2024-12-04T08:13:57.562768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def readImage(imagePath):\n  # Load image\n  img = cv2.imread(imagePath)\n  # img = cv2.resize(img, (256, 256))\n  \n  f1 = get_mag(np.fft.fft2(img[:,:,0]))\n  f2 = get_mag(np.fft.fft2(img[:,:,1]))\n  f3 = get_mag(np.fft.fft2(img[:,:,2]))  \n  \n  # result = torch.stack([f1,f2,f2,torch.tensor(img[:,:,0]),torch.tensor(img[:,:,1]),torch.tensor(img[:,:,2])],0)\n  result = torch.stack([f1,f2,f2],0)\n  return result","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:13:58.052315Z","iopub.execute_input":"2024-12-04T08:13:58.052614Z","iopub.status.idle":"2024-12-04T08:13:58.057711Z","shell.execute_reply.started":"2024-12-04T08:13:58.052593Z","shell.execute_reply":"2024-12-04T08:13:58.056831Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass buildDataset(Dataset):\n    def __init__(self,rootFolder):\n        self.rootFolder = rootFolder\n        self.images = []\n        print(\"yo\")\n        for f in os.listdir(self.rootFolder):\n#             f = os.path.join(self.rootFolder,f)\n#             print(f)\n            if f=='FAKE':\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                imgs1=[]\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    if ind>40000:\n                        break\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,0])\n                    \n            else:\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    if ind>40000:\n                        break\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,1])\n                    \n                        \n                    \n        \n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        input1 = self.images[index][0]\n        label = self.images[index][1]\n        return (input1,label)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:14:25.980407Z","iopub.execute_input":"2024-12-04T08:14:25.981215Z","iopub.status.idle":"2024-12-04T08:14:25.988565Z","shell.execute_reply.started":"2024-12-04T08:14:25.981189Z","shell.execute_reply":"2024-12-04T08:14:25.987646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass buildDataset2(Dataset):\n    def __init__(self,rootFolder):\n        self.rootFolder = rootFolder\n        self.images = []\n        print(\"yo\")\n        for f in os.listdir(self.rootFolder):\n#             f = os.path.join(self.rootFolder,f)\n#             print(f)\n            if f=='FAKE':\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                imgs1=[]\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    if ind<40000 :\n                        continue\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,0])\n                    # if ind>30000:\n                    #     break\n            else:\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    if ind<40000:\n                        continue\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,1])\n                    # if ind>30000:\n                    #     break\n                        \n                    \n        \n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        input1 = self.images[index][0]\n        label = self.images[index][1]\n        return (input1,label)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:14:26.561741Z","iopub.execute_input":"2024-12-04T08:14:26.562521Z","iopub.status.idle":"2024-12-04T08:14:26.569955Z","shell.execute_reply.started":"2024-12-04T08:14:26.562494Z","shell.execute_reply":"2024-12-04T08:14:26.569009Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = buildDataset('/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train')\ndataset_size = len(dataset)\ntrainDs = DataLoader(dataset,16,shuffle = True,pin_memory = False)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:14:27.102920Z","iopub.execute_input":"2024-12-04T08:14:27.103608Z","iopub.status.idle":"2024-12-04T08:25:48.812584Z","shell.execute_reply.started":"2024-12-04T08:14:27.103582Z","shell.execute_reply":"2024-12-04T08:25:48.811752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = buildDataset('/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train')\ndataset_size = len(dataset)\nvalDs = DataLoader(dataset,16,shuffle = True,pin_memory = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:25:48.813987Z","iopub.execute_input":"2024-12-04T08:25:48.814243Z","iopub.status.idle":"2024-12-04T08:29:55.812009Z","shell.execute_reply.started":"2024-12-04T08:25:48.814224Z","shell.execute_reply":"2024-12-04T08:29:55.811074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n# device = 'cpu'\nmodel = Model(16,0,2,3,8,8,32)#emb_size,drop_out,target must be den n_classes,in_channels,patch size,image_size-14,0, 2,1,1,16,,,torch.load('tuckerTransformerModel_Stargan_2.pth')#\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=0.00001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:29:55.813123Z","iopub.execute_input":"2024-12-04T08:29:55.813373Z","iopub.status.idle":"2024-12-04T08:29:55.858871Z","shell.execute_reply.started":"2024-12-04T08:29:55.813353Z","shell.execute_reply":"2024-12-04T08:29:55.857971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trainModel(model,criterion,optimizer,epochs,trainDs,valDs,path,patience=5):\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    batch_train_loss = []\n    batch_train_acc = []\n    batch_val_acc = []\n    batch_val_loss = []\n    early_stopping = EarlyStopping(patience=patience, verbose=True,delta = 0.01 , path=path)\n\n    best_model_wts = copy.deepcopy(model.state_dict()) \n    max_acc=0\n    for epoch in tqdm(range(epochs)):\n        model.train()\n        current_corrects = 0.0\n        train_loss=[]\n        for batchNum, (inputs1, labels) in enumerate(trainDs):\n            input1 = inputs1.to(device).float()\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(input1)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss.append(loss.item())\n            current_corrects += torch.sum(preds == labels.data)\n        \n\n        \n        train_acc = current_corrects.double() / len(trainDs.sampler)\n        batch_train_loss.append(np.mean(train_loss))\n        batch_train_acc.append(train_acc)\n        \n        model.eval()\n        current_corrects = 0.0\n        val_loss=[]\n        \n        \n        for batchNum, (inputs1, labels) in enumerate(valDs):\n            input1 = inputs1.to(device).float()\n            labels = labels.to(device)\n            outputs = model(input1)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            val_loss.append(loss.item())\n            current_corrects += torch.sum(preds == labels.data)\n\n        val_acc = current_corrects.double() / len(valDs.sampler)\n        batch_val_loss.append(np.mean(val_loss))\n        batch_val_acc.append(val_acc)\n        if max_acc < val_acc:\n            best_model_wts = copy.deepcopy(model.state_dict()) \n            \n        early_stopping(-1*val_acc, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break     \n        \n        print('Epoch Number {}: Train- Loss{:.4f} Acc: {:.4f}'.format(epoch,batch_train_loss[-1],batch_train_acc[-1]))\n        print('Epoch Number {}: Val- Loss{:.4f} Acc: {:.4f}'.format(epoch,batch_val_loss[-1],batch_val_acc[-1]))               \n        \n   \n    return model,batch_train_loss,batch_train_acc,batch_val_loss,batch_val_acc,best_model_wts     \t\t\t\t\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:29:55.861194Z","iopub.execute_input":"2024-12-04T08:29:55.861841Z","iopub.status.idle":"2024-12-04T08:29:55.872418Z","shell.execute_reply.started":"2024-12-04T08:29:55.861809Z","shell.execute_reply":"2024-12-04T08:29:55.871736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\ndef readImage2(imagePath):\n    # Load the image\n    img = cv2.imread(imagePath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for visualization\n\n    # Compute FFT and magnitude for each channel\n    f1 = get_mag(np.fft.fft2(img[:, :, 0], s=(256, 256)))\n    f2 = get_mag(np.fft.fft2(img[:, :, 1], s=(256, 256)))\n    f3 = get_mag(np.fft.fft2(img[:, :, 2], s=(256, 256)))\n\n\n    # Stack the FFT magnitudes\n    result = torch.stack([f1, f2, f3], 0)\n\n    # Visualization\n    plt.figure(figsize=(12, 6))\n\n    # Original image\n    plt.subplot(2, 4, 1)\n    plt.imshow(img)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n\n    # Magnitude spectra for each channel\n    plt.subplot(2, 4, 2)\n    plt.imshow(torch.log(1 + f1).numpy(), cmap=\"gray\")  # Log scale for better visibility\n    plt.title(\"FFT (Channel R)\")\n    plt.axis(\"off\")\n\n    plt.subplot(2, 4, 3)\n    plt.imshow(torch.log(1 + f2).numpy(), cmap=\"gray\")\n    plt.title(\"FFT (Channel G)\")\n    plt.axis(\"off\")\n\n    plt.subplot(2, 4, 4)\n    plt.imshow(torch.log(1 + f3).numpy(), cmap=\"gray\")\n    plt.title(\"FFT (Channel B)\")\n    plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:29:55.873257Z","iopub.execute_input":"2024-12-04T08:29:55.873464Z","iopub.status.idle":"2024-12-04T08:29:55.892983Z","shell.execute_reply.started":"2024-12-04T08:29:55.873447Z","shell.execute_reply":"2024-12-04T08:29:55.892250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"readImage2(\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/1000 (10).jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:29:55.893929Z","iopub.execute_input":"2024-12-04T08:29:55.894218Z","iopub.status.idle":"2024-12-04T08:29:56.355413Z","shell.execute_reply.started":"2024-12-04T08:29:55.894189Z","shell.execute_reply":"2024-12-04T08:29:56.354606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model,train_loss,train_acc,val_loss,val_acc,wts = trainModel(model,criterion,optimizer,50,trainDs,valDs,'/kaggle/working/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:29:56.356317Z","iopub.execute_input":"2024-12-04T08:29:56.356540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract weights\nwts = model.get_weights()\n\n# Save weights using pickle\nwith open(file_path, 'wb') as file:\n    pickle.dump(wts, file)\n\n# Load weights with pickle and set them to the model\n# with open(file_path, 'rb') as file:\n#     wts = pickle.load(file)\n# model.set_weights(wts)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-04T08:12:08.772129Z","iopub.execute_input":"2024-12-04T08:12:08.772480Z","iopub.status.idle":"2024-12-04T08:12:08.804090Z","shell.execute_reply.started":"2024-12-04T08:12:08.772444Z","shell.execute_reply":"2024-12-04T08:12:08.802886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass buildDataset3(Dataset):\n    def __init__(self,rootFolder):\n        self.rootFolder = rootFolder\n        self.images = []\n        print(\"yo\")\n        for f in os.listdir(self.rootFolder):\n#             f = os.path.join(self.rootFolder,f)\n#             print(f)\n            if f=='FAKE':\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                imgs1=[]\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,0])\n            else:\n                f = os.path.join(self.rootFolder,f)\n#             print(f)\n                ind=0\n                for im in tqdm(os.listdir(f)):\n                    ind+=1\n                    im = os.path.join(f,im)\n                    img = readImage(im)\n                    self.images.append([img,1])\n                        \n                    \n        \n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        input1 = self.images[index][0]\n        label = self.images[index][1]\n        return (input1,label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:40:29.457898Z","iopub.status.idle":"2024-12-04T07:40:29.458178Z","shell.execute_reply.started":"2024-12-04T07:40:29.458043Z","shell.execute_reply":"2024-12-04T07:40:29.458055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = buildDataset3('/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test')\ndataset_size = len(dataset)\ntestDs = DataLoader(dataset,16,shuffle = True,pin_memory = False)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.459537Z","iopub.status.idle":"2024-12-04T07:40:29.459980Z","shell.execute_reply.started":"2024-12-04T07:40:29.459768Z","shell.execute_reply":"2024-12-04T07:40:29.459786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\nmodel.eval()\ncurrent_corrects = 0.0\nval_loss = []\nall_preds = []\nall_labels = []\n\nfor batchNum, (inputs1, labels) in enumerate(testDs):\n    input1 = inputs1.to(device).float()\n    labels = labels.to(device)\n    outputs = model(input1)\n    _, preds = torch.max(outputs, 1)\n    \n    # Store predictions and labels for evaluation\n    all_preds.extend(preds.cpu().numpy())\n    all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate loss and corrects\n    loss = criterion(outputs, labels)\n    val_loss.append(loss.item())\n    current_corrects += torch.sum(preds == labels.data)\n\n# Calculate accuracy\nval_acc = current_corrects.double() / len(testDs.sampler)\nprint(\"Test Accuracy:\", val_acc)\n\n# Calculate and print the confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1])\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Calculate precision, recall, F1-score, and support for each class\nclass_report = classification_report(all_labels, all_preds, labels=[0, 1], target_names=[\"Class 0\", \"Class 1\"])\nprint(\"Classification Report:\")\nprint(class_report)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:40:29.460880Z","iopub.status.idle":"2024-12-04T07:40:29.461141Z","shell.execute_reply.started":"2024-12-04T07:40:29.461014Z","shell.execute_reply":"2024-12-04T07:40:29.461025Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}