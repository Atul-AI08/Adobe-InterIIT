{
    "model_name_or_path": "google/paligemma-3b-mix-224",
    "do_train": true,
    "dataset": "custom",
    "template": "paligemma",
    "finetuning_type": "lora",
    "lora_target": "all",
    "output_dir": "paligemma_lora",
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 4,
    "learning_rate": 5e-05,
    "num_train_epochs": 1
}